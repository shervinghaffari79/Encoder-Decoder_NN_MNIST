# MNIST Autoencoder Project ğŸ–¼ï¸

## Overview ğŸ“–
This project implements a convolutional autoencoder using PyTorch to compress and reconstruct images from the MNIST dataset. The autoencoder learns to encode high-dimensional input images into a lower-dimensional latent space and then decode them back to their original form, achieving effective image reconstruction with minimal loss.

## Project Structure ğŸ—‚ï¸
- <code>project_3_3.ipynb</code>: Jupyter notebook containing the complete implementation, including data preprocessing, model architecture, training, and visualization. ğŸ““
- <code>/data</code>: Directory where the MNIST dataset is automatically downloaded and stored. ğŸ“
- <code>README.md</code>: This file, providing an overview and documentation of the project. ğŸ“„

## Table of Contents ğŸ“‘
- [Methodology](#methodology) ğŸ› ï¸
- [Achievements](#achievements) ğŸ†
- [Installation and Setup](#installation-and-setup) ğŸ”§
- [Steps to Run](#steps-to-run) ğŸš€
- [Usage](#usage) â–¶ï¸
- [Future Improvements](#future-improvements) ğŸ”®
- [License](#license) ğŸ“„
- [Acknowledgments](#acknowledgments) ğŸ™

## Methodology ğŸ› ï¸
The project follows a structured approach to build, train, and evaluate a convolutional autoencoder for the MNIST dataset. Below is a detailed breakdown of the methodology:

### 1. Data Preparation ğŸ“Š
- **Dataset**: The MNIST dataset, consisting of 60,000 grayscale handwritten digit images (28x28 pixels) for training, is used. âœï¸
- **Preprocessing**: Images are transformed into PyTorch tensors using <code>transforms.ToTensor()</code> and normalized to the range [0, 1]. ğŸ”„
- **Data Loading**: The dataset is loaded using <code>torchvision.datasets.MNIST</code> and batched with a <code>DataLoader</code> (batch size = 32, shuffled) for efficient training. ğŸ“¥

### 2. Model Architecture ğŸ—ï¸
The autoencoder consists of two main components: an Encoder and a Decoder, combined into an <code>EncoderDecoder</code> class.

#### Encoder ğŸ”
- **Input**: Grayscale images (1x28x28). ğŸ–¼ï¸
- **Architecture**:
  - Three convolutional layers (<code>nn.Conv2d</code>) with ReLU activation:
    - Conv1: 1 input channel â†’ 16 output channels, kernel size 3, stride 2, padding 1.
    - Conv2: 16 input channels â†’ 32 output channels, kernel size 3, stride 2, padding 1.
    - Conv3: 32 input channels â†’ 64 output channels, kernel size 3, stride 2, padding 1.
  - Adaptive average pooling (<code>nn.AdaptiveAvgPool2d</code>) to reduce the feature map to a 1x1 spatial dimension, producing a 64-dimensional latent representation. ğŸ“‰
- **Purpose**: Compresses the input image into a compact latent space representation. ğŸ¤

#### Decoder ğŸ”„
- **Input**: 64-dimensional latent vector (reshaped to 64x1x1). ğŸ“
- **Architecture**:
  - Four transposed convolutional layers (<code>nn.ConvTranspose2d</code>) with ReLU activation (except the final layer):
    - ConvTranspose1: 64 input channels â†’ 32 output channels, kernel size 4, stride 1, padding 0 (outputs 4x4).
    - ConvTranspose2: 32 input channels â†’ 16 output channels, kernel size 3, stride 2, padding 1 (outputs 7x7).
    - ConvTranspose3: 16 input channels â†’ 8 output channels, kernel size 4, stride 2, padding 1 (outputs 14x14).
    - ConvTranspose4: 8 input channels â†’ 1 output channel, kernel size 4, stride 2, padding 1 (outputs 28x28).
- **Purpose**: Reconstructs the original 28x28 image from the latent representation. ğŸ–¼ï¸

#### EncoderDecoder ğŸ”—
- Combines the Encoder and Decoder into a single model, passing the encoder's output directly to the decoder. âš™ï¸

### 3. Training ğŸ‹ï¸â€â™‚ï¸
- **Loss Function**: Mean Squared Error (MSE) loss to measure the difference between the input and reconstructed images. ğŸ“
- **Optimizer**: Adam optimizer with a learning rate of 0.001. âš¡
- **Training Loop**:
  - 10 epochs, processing batches of 32 images.
  - For each batch:
    - Zero the gradients.
    - Forward pass through the autoencoder.
    - Compute MSE loss.
    - Backpropagate and update model parameters.
  - Loss is printed for each epoch to monitor training progress. ğŸ“ˆ

### 4. Evaluation and Visualization ğŸ“Š
- **Visualization**: After training, the notebook visualizes 10 pairs of images (original vs. reconstructed) from the training set using Matplotlib. ğŸ‘€
- **Metrics**: The final training loss after 10 epochs is approximately 0.0079, indicating good reconstruction quality. âœ…

## Achievements ğŸ†
- **Effective Compression**: The autoencoder successfully compresses 28x28 MNIST images (784 pixels) into a 64-dimensional latent space, achieving a compression ratio of ~12:1. ğŸ“‰
- **High Reconstruction Quality**: The model reconstructs images with minimal loss (final MSE ~0.0079), preserving key visual features of the handwritten digits. âœ¨
- **Robust Implementation**: The use of convolutional layers and adaptive pooling ensures efficient feature extraction and reconstruction, suitable for image data. ğŸ’ª
- **Clear Visualization**: The project includes clear visualizations comparing original and reconstructed images, demonstrating the model's ability to capture essential digit characteristics. ğŸ–¼ï¸

## Installation and Setup ğŸ”§
To run this project, ensure you have the following dependencies installed:
```bash
pip install torch torchvision matplotlib
```

## Steps to Run ğŸš€
1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   ```
2. **Navigate to the project directory**:
   ```bash
   cd <project-directory>
   ```
3. **Open and run the Jupyter notebook**:
   ```bash
   jupyter notebook project_3_3.ipynb
   ```
Ensure the <code>/data</code> directory is writable, as the MNIST dataset will be downloaded there automatically. ğŸ“‚

## Usage â–¶ï¸
- **Training**: Execute the training cells in the notebook to train the autoencoder for 10 epochs. Modify <code>batch_size</code>, <code>lr</code>, or the number of epochs as needed. ğŸ‹ï¸â€â™‚ï¸
- **Visualization**: Run the visualization cell to compare original and reconstructed images. ğŸ‘ï¸
- **Customization**: Adjust the encoder/decoder architecture (e.g., number of layers, channels) or hyperparameters to experiment with different configurations. ğŸ› ï¸

## Future Improvements ğŸ”®
- **Latent Space Analysis**: Explore the latent space to understand learned representations or use them for tasks like clustering or generation. ğŸ”
- **Advanced Architectures**: Incorporate techniques like batch normalization, dropout, or variational autoencoders for improved performance. âš™ï¸
- **Extended Evaluation**: Add quantitative metrics (e.g., SSIM, PSNR) to evaluate reconstruction quality beyond MSE. ğŸ“Š
- **Testing Set**: Include a test dataset to evaluate generalization performance. âœ…

## License ğŸ“„
This project is licensed under the MIT License. See the <a href="LICENSE">LICENSE</a> file for details. âš–ï¸

## Acknowledgments ğŸ™
- The MNIST dataset is provided by Yann LeCun and team. ğŸ“š
- Built using PyTorch and torchvision. ğŸ”¥

Happy coding and experimenting with autoencoders! ğŸ‰
